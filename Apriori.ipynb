{
  "metadata": {
    "creator": "admin",
    "kernelspec": {
      "name": "py-dku-venv-dash",
      "display_name": "Python (env dash)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "createdOn": 1636625077251,
    "tags": [],
    "customFields": {},
    "hide_input": false,
    "modifiedBy": "ajmendez@faculty.ie.edu"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import libraries and declare main arrays\n%pylab inline\n\nimport numpy as np\nimport pandas as pd\nimport dataiku\nfrom dataiku import pandasutils as pdu\nfrom itertools import combinations\n\nmetric_dict \u003d {\n    \"antecedent support\": lambda _, sA, __: sA,\n    \"consequent support\": lambda _, sA, __: sC,\n    \"confidence\": lambda sAC, sA, _: sAC/sA,\n    \"rule support\": lambda sAC, _, __: sAC,\n    \"conf.difference\": lambda sAC, sA, sC: abs(metric_dict[\"confidence\"](sAC, sA, sC)-sC),\n    \"conf.ratio\": lambda sAC, sA, sC: 1-np.minimum(sAC/sA, sC)/np.maximum(sAC/sA, sC),\n    \"lift\": lambda sAC, sA, sC: metric_dict[\"confidence\"](sAC, sA, sC)/sC\n    }\n\ncolumns_ordered \u003d [\"antecedent support\", \"consequent support\", \n                   \"confidence\", \"rule support\", \"lift\",\n                   \"conf.difference\", \"conf.ratio\"]\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apriori function\ndef apriori(df, min_support\u003d0, use_colnames\u003dFalse):\n    X \u003d df.values\n    ary_col_idx \u003d np.arange(X.shape[1]-1)+1\n    support \u003d (np.sum(X, axis\u003d0) / float(X.shape[0]))\n    support\u003dsupport[support\u003c1]\n    support_dict \u003d {1: support[support \u003e\u003d min_support]}\n    itemset_dict \u003d {1: ary_col_idx[support \u003e\u003d min_support].reshape(-1, 1)}\n    \n    max_itemset \u003d 1\n\n    while max_itemset:\n        next_max_itemset \u003d max_itemset + 1\n        combin \u003d combinations(np.unique(itemset_dict[max_itemset].flatten()),\n                              r\u003dnext_max_itemset)\n        frequent_items \u003d []\n        frequent_items_support \u003d []\n\n        for c in combin:\n            together \u003d X[:, c].sum(axis\u003d1) \u003d\u003d len(c)\n            support \u003d together.sum() / float(X.shape[0])\n            if support \u003e\u003d min_support:\n                frequent_items.append(c)\n                frequent_items_support.append(support)\n\n        if frequent_items:\n            itemset_dict[next_max_itemset] \u003d np.array(frequent_items)\n            support_dict[next_max_itemset] \u003d np.array(frequent_items_support)\n            max_itemset \u003d next_max_itemset\n        else:\n            max_itemset \u003d 0\n\n    all_res \u003d []\n    for k in sorted(itemset_dict):\n        support \u003d pd.Series(support_dict[k])\n        itemsets \u003d pd.Series([i for i in itemset_dict[k]])\n\n        res \u003d pd.concat((support, itemsets), axis\u003d1)\n        all_res.append(res)\n\n    res_df \u003d pd.concat(all_res)\n    res_df.columns \u003d [\u0027support\u0027, \u0027itemsets\u0027]\n    if use_colnames:\n        mapping \u003d {idx: item for idx, item in enumerate(df.columns)}\n        res_df[\u0027itemsets\u0027] \u003d res_df[\u0027itemsets\u0027].apply(lambda x: [mapping[i]\n                                                      for i in x])\n    res_df \u003d res_df.reset_index(drop\u003dTrue)\n    return res_df\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe (change the dataset name if on a different project)\nmydataset \u003d dataiku.Dataset(\"Shopping_tabular\")\ndf \u003d mydataset.get_dataframe()\ndf.head()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Area to change the script parameters\nmetric\u003d\"lift\"        # metric used for threshold\nmin_threshold\u003d1      # metric threshold\nmin_confidence\u003d0.75         # min confidence\nmin_rulesupport\u003d0       # min rule support (enter 0 to use antecedent support + other metric)\nmin_support\u003d0.1       # min antecedent support\nmax_antecedent\u003d5        # define maximum number of antecedent\n\n# calls apriori function to produce, in first hand, the most frequent combinations of products \u003d itemsets\nfreq\u003dapriori(df, min_rulesupport, use_colnames\u003dTrue)\n\nkeys \u003d freq[\u0027itemsets\u0027].values\nvalues \u003d freq[\u0027support\u0027].values\nfrozenset_vect \u003d np.vectorize(lambda x: frozenset(x))\nfrequent_items_dict \u003d dict(zip(frozenset_vect(keys), values))\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# prepare buckets to collect frequent rules\nrule_antecedents \u003d []\nrule_consequent \u003d []\nrule_supports \u003d []\n\n# nsize will store total number of tickets\nnsize\u003dlen(df.index) \n\n# iterate over all frequent itemsets\nfor k in frequent_items_dict.keys():\n    sAC \u003d frequent_items_dict[k]\n    # iterate to find all possible combinations. len(k) is itemset length\n    for idx in range(len(k)-1, 0, -1):\n        # extract supports, antecedents and consequent from rules\n        for c in combinations(k, r\u003didx):\n            antecedent \u003d frozenset(c)\n            consequent \u003d k.difference(antecedent)\n            sA \u003d frequent_items_dict[antecedent]\n            sC \u003d frequent_items_dict[consequent]\n            instances \u003d sA * nsize\n            str_antecedent\u003dstr(list(c))[2:-2].replace(\"\u0027, u\u0027\", \",\")\n            str_consequent\u003dstr(list(consequent))[2:-2].replace(\"\u0027, u\u0027\", \",\")\n            # check thresholds\n            if metric_dict[metric](sAC, sA, sC) \u003e\u003d min_threshold:\n                if metric_dict[\u0027antecedent support\u0027](sAC, sA, sC) \u003e\u003d min_support:\n                    if metric_dict[\u0027confidence\u0027](sAC, sA, sC) \u003e\u003d min_confidence:\n                        if len(antecedent)\u003c\u003dmax_antecedent and len(consequent)\u003d\u003d1:\n                            rule_antecedents.append(str_antecedent.replace(\"\u0027,\u0027\", \",\").replace(\"\u0027, \u0027\", \",\"))\n                            rule_consequent.append(str_consequent.replace(\"\u0027,\u0027\", \",\").replace(\"\u0027, \u0027\", \",\"))\n                            rule_supports.append([len(antecedent), len(consequent), instances, sAC, sA, sC])\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "# if rules were found, generate metrics from rule_supports array\nif len(rule_supports)\u003d\u003d0:\n    print (\"EMPTY: No rules were found with the criteria specified.\")    \nelse:\n    rule_supports\u003d np.array(rule_supports).T.astype(float)\n    numbantec \u003d rule_supports[0]\n    numbconseq \u003d rule_supports[1]\n    instances \u003d rule_supports[2]\n    sAC \u003d rule_supports[3]\n    sA \u003d rule_supports[4]\n    sC \u003d rule_supports[5]\n\n    # create the final pandas dataframe: dfrules, to store rule list\n    dfrules \u003d pd.DataFrame(data\u003dlist(zip(rule_antecedents, rule_consequent, instances)), columns\u003d[\"antecedents\", \"consequent\", \"instances\"])\n\n    # add other metrics\n    for m in columns_ordered:\n        dfrules[m] \u003d metric_dict[m](sAC, sA, sC)\n\n    # add number of antecedents and number of consequents\n    dfrules2\u003dpd.DataFrame(data\u003dlist(zip(numbantec, numbconseq)), columns\u003d[\"#antec\", \"#conseq\"])\n    dfrules\u003dpd.concat([dfrules, dfrules2], axis\u003d1)\n    \n    # sort by one metric\n    dfrules\u003ddfrules.sort_values(\u0027confidence\u0027, ascending\u003dFalse)\n\n    # assign row number\n    dfrules[\u0027ruleid\u0027]\u003d\u0027\u0027\n    for index in dfrules.iterrows():\n        dfrules[\u0027ruleid\u0027]\u003ddfrules.index+1\n    \n    print (str(len(dfrules)) + \" rules found\")        "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# show rules\ndfrules"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## dfrules"
      ]
    }
  ]
}